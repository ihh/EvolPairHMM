# -*- coding: utf-8 -*-
"""GGI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dQM1iaeQghByTt69hkaQcOaedCBa8s5Y
"""

import numpy as np

import jax
import jax.numpy as jnp

from jax import grad, vmap
from jax.scipy.special import logsumexp, gammaln
from jax.scipy.linalg import expm

from functools import partial
from jax import jit

!pip install jaxtyping
from jaxtyping import Array, Float, PyTree

!pip install diffrax
from diffrax import diffeqsolve, ODETerm, Dopri5, PIDController, SaveAt

!pip install jaxopt
import jaxopt

# helper to calculate SI and SD
def SID (t: Float,
         eventRate: Float,
         extendProb: Float):
  return jnp.exp(eventRate*t/(1-extendProb)) - 1

# calculate transition matrix of F from counts Txy
def Ftransitions_from_counts (t: Float,
                              T: Float[Array, "4"],
                              params: Float[Array, "4"]):
  lam,mu,x,y = params
  # Guard against NaN gradients from division by zero
  # https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where
  SI = jnp.where (t > 0, SID (t, lam, x), 1)
  SD = jnp.where (t > 0, SID (t, mu, y), 1)
  [TMM,TMI,TIM,TDI] = T
  return jnp.where (t > 0,
           jnp.array ([[TMM, TMI, 1-TMM-TMI],
                      [TIM/SI, (SI-TMI-TDI)/SI, (TMI+TDI-TIM)/SI],
                      [(1-TMM-TIM)/SD, TDI/SD, (SD+TMM+TIM-TDI-1)/SD]]),
           jnp.array ([[1,0,0],[1-x,x,0],[1-y,0,y]]))

# calculate derivatives of counts Txy
def Tderivs (t: Float,
             T: Float[Array, "4"],
             params: Float[Array, "4"]):
  lam,mu,x,y = params
  [[a,b,c],[f,g,h],[p,q,r]] = Ftransitions_from_counts(t,T,params)
  return jnp.array ((mu*b*f*(1-y)/(1-g*y) - (lam+mu)*a,
                    -mu*b*(1-g)/(1-g*y) + lam*(1-b),
                    lam*a - mu*f*(1-g)*(b*(1-r)+c*q)/((1-g*y)*(f*(1-r)+h*p)),
                    mu*(1-g)*(b*(1-r-h*q)+c*g*q)/((1-g*y)*(f*(1-r)+h*p))))

# Handcoded RK4 and Euler for debugging only... diffrax is way cooler
def RK4next(t: Float,
            T: Float[Array, "4"],
            params: Float[Array, "4"],
            dt: Float):
  k1 = Tderivs(t, T, params)
  k2 = Tderivs(t+dt/2, T + dt*k1/2, params)
  k3 = Tderivs(t+dt/2, T + dt*k2/2, params)
  k4 = Tderivs(t+dt, T + dt*k3, params)
  return T + dt*(k1 + 2*k2 + 2*k3 + k4)/6

def EulerNext(t: Float,
             T: Float[Array, "4"],
             params: Float[Array, "4"],
             dt: Float):
  return T + dt*Tderivs(t, T, params)

# calculate transition matrix by numerical integration
# The states are M(0), I(1), D(2)
def Ftransitions (t: Float,
                  params: Float[Array, "4"]):
  lam,mu,x,y = params
  term = ODETerm(Tderivs)
  solver = Dopri5()
  stepsize_controller = PIDController(rtol=1e-5, atol=1e-5)
  T0 = jnp.array([1,0,0,0])
  dt0_rel = .01
  dt0 = jnp.maximum (dt0_rel / jnp.maximum (lam, mu), dt0_rel)
  sol = diffeqsolve (term, solver, 0, t, dt0, T0, args=params,
                     stepsize_controller=stepsize_controller,
                     )
  T1 = sol.ys[-1]
  t1 = sol.ts[-1]
  return Ftransitions_from_counts (t1, T1, params)

# Numerical calculation of time derivatives using matrix inversion
# An alternative, more general method of finding the ODEs that should work for more complex models
# Here we implement it for GGI to compare to the symbolic ODEs

# First a version of Ftransitions_from_counts that doesn't assume we know SI and SD,
# but instead uses the full 3x3 matrix of Txy counts (instead of just TMM,TMI,TIM,TDI)
# NB the 3x3 matrix is shaped as a 9x1 vector
def Ftransitions_from_counts_3x3 (t: Float,
                                  T: Float[Array, "9"],
                                  params: Float[Array, "4"]):
  lam,mu,x,y = params
  # Guard against NaN gradients from division by zero
  # https://jax.readthedocs.io/en/latest/faq.html#gradients-contain-nan-where-using-where
  [TMM,TMI,TMD,TIM,TII,TID,TDM,TDI,TDD] = T
  SM = TMM + TMI + TMD
  SI = TIM + TII + TID
  SD = TDM + TDI + TDD
  return jnp.where (t > 0,
           jnp.array ([[TMM/SM, TMI/SM, TMD/SM],
                       [TIM/SI, TII/SI, TID/SI],
                       [TDM/SD, TDI/SD, TDD/SD]]),
           jnp.array ([[1,0,0],[1-x,x,0],[1-y,0,y]]))

# The constant and dt-coefficient parts of the composite transition matrix
def calc_U0 (Ftransitions: Float[Array, "3 3"],
             params: Float[Array, "4"]):
  [[a,b,c],[f,g,h],[p,q,r]] = Ftransitions
  lam,mu,x,y = params
  return jnp.array ([[a,0,b,0,0,c,0,0,0],
                     [a*(1-x),x,b*(1-x),0,0,0,0,c,0],
                     [f,0,g,0,0,h,0,0,0],
                     [f*(1-x),0,g*(1-x),x,0,0,0,h,0],
                     [a*(1-y),0,b*(1-y),0,a*y,0,c,0,b*y],
                     [p,0,q,0,0,r,0,0,0],
                     [p*(1-y),0,q*(1-y),0,p*y,0,r,0,q*y],
                     [p*(1-x),0,q*(1-x),0,0,0,0,r,0],
                     [f*(1-y),0,g*(1-y),0,f*y,0,h,0,g*y]])

def calc_Ut (Ftransitions: Float[Array, "3 3"],
             params: Float[Array, "4"]):
  [[a,b,c],[f,g,h],[p,q,r]] = Ftransitions
  lam,mu,x,y = params
  return jnp.array ([[-a*(lam+mu),lam,-b*(lam+mu),0,a*mu,0,0,0,b*mu],
                     [0,0,0,0,0,0,0,0,0],
                     [-f*(lam+mu),0,-g*(lam+mu),lam,f*mu,0,0,0,g*mu],
                     [0,0,0,0,0,0,0,0,0],
                     [0,0,0,0,0,0,0,0,0],
                     [-p*(lam+mu),0,-q*(lam+mu),0,p*mu,0,0,0,q*mu],
                     [0,0,0,0,0,0,0,0,0],
                     [0,0,0,0,0,0,0,0,0],
                     [0,0,0,0,0,0,0,0,0]])

I9 = jnp.identity (9)
ones9 = jnp.ones (9)
indices_3x3 = jnp.indices((3,3)).reshape(2,9).transpose()

JStateTypeMask = jnp.array ([[1,0,0,0,0,0,0,0,0],  # M
                             [0,1,1,1,0,0,0,0,0],  # I
                             [0,0,0,0,1,1,1,1,0],  # D
                             [0,0,0,0,0,0,0,0,1]]) # N

JTransTypeMasks = vmap (lambda ij: jnp.outer (JStateTypeMask[ij[0]], JStateTypeMask[ij[1]])) (indices_3x3)
JTransNMask = jnp.tile (JStateTypeMask[3], (9,1))
JTransLMask = jnp.tile (ones9 - JStateTypeMask[0], (9,1))
JTransRMask = JTransLMask.transpose()

# calculate derivatives of counts Txy
def calc_V0 (J, U0):
  return jnp.linalg.inv (I9 - (J * U0))

def Txy_deriv (Jxy, args):
  U0,Ut,V0L1,VtL1,V0NU0,VtNU0_plus_V0NUt,V0R1,VtR1 = args
  V0xy = calc_V0(Jxy,U0)
  JxyUt = Jxy * Ut
  JxyV0NU0 = Jxy * V0NU0
  return VtL1 @ JxyV0NU0 @ V0R1 + V0L1 @ (Jxy * VtNU0_plus_V0NUt) @ V0R1 + V0L1 @ JxyV0NU0 @ VtR1

vmap_Txy_deriv = vmap (Txy_deriv, in_axes=(0,None))

def Tderivs_3x3 (t: Float,
                 T: Float[Array, "9"],
                 params: Float[Array, "4"]):
  Ftrans = Ftransitions_from_counts_3x3(t,T,params)
  U0 = calc_U0 (Ftrans, params)
  Ut = calc_Ut (Ftrans, params)
  V0L, V0N, V0R = calc_V0(JTransLMask,U0), calc_V0(JTransNMask,U0), calc_V0(JTransRMask,U0)
  JLUt, JRUt, JNUt = JTransLMask * Ut, JTransRMask * Ut, JTransNMask * Ut
  V0L1, V0R1 = V0L[0], V0R[:,0]
  VtL1 = V0L1 @ JLUt @ V0L
  VtR1 = V0R @ JRUt @ V0R1
  VtN = V0N @ JNUt @ V0N
  V0NU0 = V0N @ U0
  VtNU0_plus_V0NUt = VtN @ U0 + V0N @ Ut
  args = U0,Ut,V0L1,VtL1,V0NU0,VtNU0_plus_V0NUt,V0R1,VtR1
  return vmap_Txy_deriv (JTransTypeMasks, args)

def Ftransitions_3x3 (t: Float,
                      params: Float[Array, "4"]):
  lam,mu,x,y = params
  term = ODETerm(Tderivs_3x3)
  solver = Dopri5()
  stepsize_controller = PIDController(rtol=1e-5, atol=1e-5)
  T0 = jnp.array([1,0,0,0,0,0,0,0,0])
  dt0_rel = .01
  dt0 = jnp.maximum (dt0_rel / jnp.maximum (lam, mu), dt0_rel)
  sol = diffeqsolve (term, solver, 0, t, dt0, T0, args=params,
                     stepsize_controller=stepsize_controller,
                     )
  T1 = sol.ys[-1]
  t1 = sol.ts[-1]
  return Ftransitions_from_counts_3x3 (t1, T1, params)

# TKF91 special case
def Ftransitions_TKF91 (t: Float,
                        lam: Float,
                        mu: Float):
  elt = jnp.exp (-lam*t)
  emt = jnp.exp (-mu*t)
  a = emt
  b = lam * (elt - emt) / (mu*elt - lam*emt)
  g = 1 - mu*b/(lam*(1-a))
  return jnp.array([[(1-b)*a,b,(1-b)*(1-a)],
                    [(1-b)*a,b,(1-b)*(1-a)],
                    [(1-g)*a,g,(1-g)*(1-a)]])

def get_eqm (submat):
  large_t = 1 / jnp.min(jnp.abs(submat)[jnp.nonzero(submat,size=1)])
  return jnp.matmul (expm (submat * large_t), jnp.ones (submat.shape[0]))

def normalize_rate_matrix(mx):
  idx = range(mx.shape[0])
  def normrow(mx,i):
    return [mx[i,j] - (jnp.sum(mx[i,:]) if i==j else 0) for j in idx]
  return jnp.array ([normrow(mx,i) for i in idx])

def hky85 (eqm, ti, tv):
  idx = range(4)
  raw = [[eqm[j] * (ti if i & 1 == j & 1 else tv) for j in idx] for i in idx]
  return normalize_rate_matrix (jnp.array (raw))

def dsq(str,alph):
  return jnp.array ([alph.index(c) for c in list(str)])

def onehot(str,alph):
  R = range(len(alph))
  char2onehot = {alph[i]: [1. if j==i else 0. for j in R] for i in R}
  return jnp.array ([char2onehot[c] for c in list(str)])

# impure (but legible) reference implementation of Forward algorithm
# x is the ancestor, its index is i
# y is the descendant, its index is j
# The states are M(0), I(1), D(2)
def forward_impure_1hot (x, y, transmat, submat, pi):
  [[a,b,c],[f,g,h],[p,q,r]] = jnp.log (transmat)
  lsm = jnp.log (submat) - jnp.log (pi)
#  print ("trans:", jnp.log (transmat))
#  print ("subst:", lsm)
  xs, ys = len(x) + 1, len(y) + 1
  F = np.full ((xs, ys, 3), -np.Infinity)
  F[0,0,0] = 0
  for i in range(xs):
    for j in range(ys):
      if (i > 0 and j > 0):
        F[i,j,0] = logsumexp (jnp.array ([F[i-1,j-1,0] + a,
                                          F[i-1,j-1,1] + f,
                                          F[i-1,j-1,2] + p])
                              + jnp.matmul (jnp.matmul (x[i-1], lsm), y[j-1]))
      if (j > 0):
        F[i,j,1] = logsumexp (jnp.array ([F[i,j-1,0] + b,
                                          F[i,j-1,1] + g,
                                          F[i,j-1,2] + q]))
      if (i > 0):
        F[i,j,2] = logsumexp (jnp.array ([F[i-1,j,0] + c,
                                          F[i-1,j,1] + h,
                                          F[i-1,j,2] + r]))
#  print(F)
  return logsumexp (jnp.array ([F[xs-1,ys-1,0] + logsumexp(jnp.array ([a,c])),
                     F[xs-1,ys-1,1] + logsumexp(jnp.array ([f,h])),
                     F[xs-1,ys-1,2] + logsumexp(jnp.array ([p,r]))]))

# null model sequence (log) probability
def null_model_prob_1hot (seq_1hot, pi):
  return jnp.sum (jnp.dot (seq_1hot, jnp.log(pi)))

def null_model_prob (dsq, pi):
  return null_model_prob (jax.nn.one_hot(dsq, pi.shape[0]), pi)

def dummy_null_model (seq, pi):
  return 0.

# impure (but legible) reference implementation of Forward algorithm
# x is the ancestor, its index is i
# y is the descendant, its index is j
# The states are M(0), I(1), D(2)
def forward_impure (x, y, transmat, submat, pi):
  [[a,b,c],[f,g,h],[p,q,r]] = jnp.log (transmat)
  lsm = jnp.log (submat) - jnp.log (pi)
#  print ("trans:", jnp.log (transmat))
#  print ("subst:", lsm)
  xs, ys = len(x) + 1, len(y) + 1
  F = np.full ((xs, ys, 3), -np.Infinity)
  F[0,0,0] = 0
  for i in range(xs):
    for j in range(ys):
      if (i > 0 and j > 0):
        F[i,j,0] = logsumexp (jnp.array ([F[i-1,j-1,0] + a,
                                          F[i-1,j-1,1] + f,
                                          F[i-1,j-1,2] + p])
                              + lsm[x[i-1]][y[j-1]])
      if (j > 0):
        F[i,j,1] = logsumexp (jnp.array ([F[i,j-1,0] + b,
                                          F[i,j-1,1] + g,
                                          F[i,j-1,2] + q]))
      if (i > 0):
        F[i,j,2] = logsumexp (jnp.array ([F[i-1,j,0] + c,
                                          F[i-1,j,1] + h,
                                          F[i-1,j,2] + r]))
#  print(F)
  return logsumexp (jnp.array ([F[xs-1,ys-1,0] + logsumexp(jnp.array ([a,c])),
                     F[xs-1,ys-1,1] + logsumexp(jnp.array ([f,h])),
                     F[xs-1,ys-1,2] + logsumexp(jnp.array ([p,r]))]))

def calc_transmat_submat_pi (t, indelParams, substRateMatrix):
  transmat = Ftransitions (t, indelParams)
  Q = normalize_rate_matrix (substRateMatrix)
  pi = get_eqm (Q)
  submat = expm (Q * t)
  return transmat, submat, pi

def forward_wrap (forwardFunc):
  def wrapped (x, y, t, indelParams, substRateMatrix, null=dummy_null_model):
    transmat, submat, pi = calc_transmat_submat_pi (t, indelParams, substRateMatrix)
    return forwardFunc (x, y, transmat, submat, pi) + null(x,pi) + null(y,pi)
  return wrapped

forward_impure_wrap = forward_wrap (forward_impure)

# pure, but inefficient, jax implementation of Forward algorithm
# x is the ancestor, laid out horizontally (i.e. each row compares all of x to a single site of y)
# y is the descendant, laid out vertically (i.e. each column compares all of y to a single site of x)
# The states are M(0), I(1), D(2)
def forward_inefficient_1hot (x, y, transmat, submat, pi):
  [[a,b,c],[f,g,h],[p,q,r]] = jnp.log (transmat)
  lsm = jnp.log (submat) - jnp.log (pi)
  def fillCell (cellCarry, col):
    M_src_cell, D_src_cell, yc = cellCarry
    I_src_cell, xc = col[0:3], col[3:]
    cell = jnp.array ([logsumexp (jnp.array ([M_src_cell[0] + a,
                                              M_src_cell[1] + f,
                                              M_src_cell[2] + p])
                        + jnp.matmul (jnp.matmul (xc, lsm), yc)),
                       logsumexp (jnp.array ([I_src_cell[0] + b,
                                              I_src_cell[1] + g,
                                              I_src_cell[2] + q])),
                       logsumexp (jnp.array ([D_src_cell[0] + c,
                                              D_src_cell[1] + h,
                                              D_src_cell[2] + r]))])
    return (I_src_cell, cell, yc), cell
  def fillRow (rowCarry, yc):
    prevRow, x = rowCarry
    firstCellInRow = jnp.array([-np.Infinity,
                                logsumexp (jnp.array ([prevRow[0,0] + b,
                                                      prevRow[0,1] + g])),
                                -np.Infinity])
    _finalCellCarry, restOfRow = jax.lax.scan (fillCell,
                                               (prevRow[0], firstCellInRow, yc),
                                               jnp.concatenate ([prevRow[1:], x], 1))
    row = jnp.concatenate ([jnp.array ([firstCellInRow]), restOfRow])
    return (row, x), row
  def fillFirstRow (prevCell, _xc):
    cell = jnp.array ([-np.Infinity,
                       -np.Infinity,
                       logsumexp (jnp.array ([prevCell[0] + c,
                                              prevCell[2] + r]))])
    return cell, cell
  firstCell = jnp.array ([0,-np.Infinity,-np.Infinity])
  _lastCellInFirstRow, restOfFirstRow = jax.lax.scan (fillFirstRow,
                                                      firstCell,
                                                      x)
  firstRow = jnp.concatenate ([jnp.array ([firstCell]), restOfFirstRow])
  lastRowCarry, _restOfRows = jax.lax.scan (fillRow,
                                            (firstRow,x),
                                            y)
  #print(jnp.concatenate([jnp.array([firstRow]),_restOfRows]))
  lastRow = lastRowCarry[0]
  return logsumexp (jnp.array ([lastRow[-1,0] + logsumexp(jnp.array ([a,c])),
                                lastRow[-1,1] + logsumexp(jnp.array ([f,h])),
                                lastRow[-1,2] + logsumexp(jnp.array ([p,r]))]))

# pure, but inefficient, jax implementation of Forward algorithm
# x is the ancestor, laid out horizontally (i.e. each row compares all of x to a single site of y)
# y is the descendant, laid out vertically (i.e. each column compares all of y to a single site of x)
# The states are M(0), I(1), D(2)
def forward_inefficient (x, y, transmat, submat, pi):
  [[a,b,c],[f,g,h],[p,q,r]] = jnp.log (transmat)
  lsm = jnp.log (submat) - jnp.log (pi)
  def fillCell (cellCarry, col):
    M_src_cell, D_src_cell, yc = cellCarry
    I_src_cell, xc = col
    cell = jnp.array ([logsumexp (jnp.array ([M_src_cell[0] + a,
                                              M_src_cell[1] + f,
                                              M_src_cell[2] + p]))
                        + lsm[xc][yc],
                       logsumexp (jnp.array ([I_src_cell[0] + b,
                                              I_src_cell[1] + g,
                                              I_src_cell[2] + q])),
                       logsumexp (jnp.array ([D_src_cell[0] + c,
                                              D_src_cell[1] + h,
                                              D_src_cell[2] + r]))])
    return (I_src_cell, cell, yc), cell
  def fillRow (rowCarry, yc):
    prevRow, x = rowCarry
    firstCellInRow = jnp.array([-np.Infinity,
                                logsumexp (jnp.array ([prevRow[0,0] + b,
                                                      prevRow[0,1] + g])),
                                -np.Infinity])
    _finalCellCarry, restOfRow = jax.lax.scan (fillCell,
                                               (prevRow[0], firstCellInRow, yc),
                                               (prevRow[1:], x))
    row = jnp.concatenate ([jnp.array ([firstCellInRow]), restOfRow])
    return (row, x), row
  def fillFirstRow (prevCell, _xc):
    cell = jnp.array ([-np.Infinity,
                       -np.Infinity,
                       logsumexp (jnp.array ([prevCell[0] + c,
                                              prevCell[2] + r]))])
    return cell, cell
  firstCell = jnp.array ([0,-np.Infinity,-np.Infinity])
  _lastCellInFirstRow, restOfFirstRow = jax.lax.scan (fillFirstRow,
                                                      firstCell,
                                                      x)
  firstRow = jnp.concatenate ([jnp.array ([firstCell]), restOfFirstRow])
  lastRowCarry, _restOfRows = jax.lax.scan (fillRow,
                                            (firstRow,x),
                                            y)
  #print(jnp.concatenate([jnp.array([firstRow]),_restOfRows]))
  lastRow = lastRowCarry[0]
  return logsumexp (jnp.array ([lastRow[-1,0] + logsumexp(jnp.array ([a,c])),
                                lastRow[-1,1] + logsumexp(jnp.array ([f,h])),
                                lastRow[-1,2] + logsumexp(jnp.array ([p,r]))]))

forward_inefficient_wrap = forward_wrap (forward_inefficient)
forward_inefficient_wrap_grad = grad (forward_inefficient_wrap, [2,3,4])

# Binomial coefficient using gamma functions
def log_binom (x: Float, y: Float):
  return gammaln(x+1) - gammaln(y+1) - gammaln(x-y+1)

# Returns the (log of the) probability of seeing a particular size of gap
def gap_prob (nDeletions, nInsertions, transmat):
  [[a,b,c],[f,g,h],[p,q,r]] = transmat
  log = jnp.log
  def Ck (k : int):
    logbinom = log_binom(jnp.where(nDeletions>k,nDeletions-1,k),k-1) + log_binom(jnp.where(nInsertions>k,nInsertions-1,k),k-1) # guard against out-of-range errors
    log_arg = b*(nInsertions-k)*(r*f*k + h*p*(nDeletions-k)) + c*(nDeletions-k)*(g*p*k + q*f*(nInsertions-k)) # guard against log(0)
    return jnp.exp (k * log(h*q/(g*r)) + logbinom - 2*log(k) + log(jnp.where(log_arg>0,log_arg,1)))
  return jnp.where (nDeletions == 0,
                    jnp.where (nInsertions == 0,
                               log(a),
                               log(b) + (nInsertions - 1)*log(g) + log(f)),
                    jnp.where (nInsertions == 0,
                               log(c) + (nDeletions - 1)*log(r) + log(p),
                               (nDeletions - 1)*log(g) + (nInsertions-1)*log(r)
                               + log (b*h*p + c*q*f + sum ([jnp.where ((k<nInsertions or k<nDeletions) and k<=nInsertions and k<=nDeletions,
                                                                       Ck(k),
                                                                       0) for k in range(1,nDeletions+nInsertions)]))))

# Impure reference implementation using Forward algorithm
def gap_prob_impure_dp (nDeletions, nInsertions, transmat):
  [[a,b,c],[f,g,h],[p,q,r]] = jnp.log (transmat)
  xs, ys = nDeletions + 1, nInsertions + 1
  F = np.full ((xs, ys, 3), -np.Infinity)
  F[0,0,0] = 0
  for i in range(xs):
    for j in range(ys):
      if (j > 0):
        F[i,j,1] = logsumexp (jnp.array ([F[i,j-1,0] + b,
                                          F[i,j-1,1] + g,
                                          F[i,j-1,2] + q]))
      if (i > 0):
        F[i,j,2] = logsumexp (jnp.array ([F[i-1,j,0] + c,
                                          F[i-1,j,1] + h,
                                          F[i-1,j,2] + r]))
  return logsumexp (jnp.array ([F[xs-1,ys-1,0] + a,
                     F[xs-1,ys-1,1] + f,
                     F[xs-1,ys-1,2] + p]))

def is_gap (c):
  return c == '.' or c == '-'

def summarize_alignment (xstr, ystr, alph):
  if len(xstr) != len(ystr):
    raise Exception ("Alignment strings must have same length")
  ni = nd = 0
  subCount = {}
  gapCount = {}
  def inc (dict, x, y):
    if x >= 0 and y >= 0:
      key = str(x) + " " + str(y)
      dict[key] = dict[key] + 1 if key in dict else 1
  def dict2array (dict):
    return [[int(s) for s in key.split()] + [dict[key]] for key in dict.keys()]
  for i in range(len(xstr)):
    xc,yc = xstr[i], ystr[i]
    if not is_gap(xc) and not is_gap(yc):
      inc (subCount, alph.index(xc), alph.index(yc))
      inc (gapCount, nd, ni)
      ni = nd = 0
    else:
      if not is_gap(xc):
        nd = nd + 1
      if not is_gap(yc):
        ni = ni + 1
  inc (gapCount, nd, ni)
  return dict2array(subCount), dict2array(gapCount)

def alignment_likelihood (alignmentSummary, t, indelParams, substRateMatrix, conditional=True):
  subCounts, gapCounts = alignmentSummary
  transmat, submat, pi = calc_transmat_submat_pi (t, indelParams, substRateMatrix)
  log_pi = jnp.log(pi)
  def sub_loglike (x_y_count):
    x, y, count = x_y_count
    return count * (jnp.log(submat[x][y]) + jnp.where (conditional, -log_pi[y], log_pi[x]))
  def gap_loglike (i_j_count):
    i, j, count = i_j_count
    return count * gap_prob (i, j, transmat)
  return sum ([sub_loglike(c) for c in subCounts]) + sum ([gap_loglike(c) for c in gapCounts])

# workaround for lack of geometric distribution in older jax.random
def geometric (prng, p):
  return int(jnp.ceil(jnp.log(jax.random.uniform(prng)) / jnp.log1p(-p)))

# this is a weird mix of jax, numpy, and basic Python because I gave up on making it differentiable halfway through writing it :-\
def simulate (rngSeed, initLen, t, indelParams, substRateMatrix):
  Q = normalize_rate_matrix (substRateMatrix)
  log_pi = jnp.log (get_eqm (Q))
  log_submat = jnp.log (expm (Q * t))
  lam,mu,x,y = indelParams
  ancKey,desKey,rngKey = jax.random.split (jax.random.PRNGKey (rngSeed), num=3)
  anc = jax.random.categorical (ancKey, log_pi, shape=(initLen,))
  des = jax.random.categorical (desKey, vmap(lambda i:log_submat[i,:])(anc))
  ancPos = jnp.arange (des.shape[0])
  seq = jnp.array([des,ancPos]).transpose()
  while t > 0:
    timeKey, posKey, lenKey, insKey, nextKey = jax.random.split (rngKey, num=5)
    seqLen = seq.shape[0]
    totalInsRate = (seqLen+1) * lam
    totalDelRate = seqLen * mu
    totalRate = totalInsRate + totalDelRate
    posRand = jax.random.uniform (posKey) * totalRate
    isInsert = posRand < totalInsRate
    pos = int (jnp.floor (jnp.where (isInsert,
                                     posRand / lam,
                                     (posRand - totalInsRate) / mu)))
    evtLen = 1 + geometric (lenKey, 1 - jnp.where(isInsert,x,y))
    insProbs = np.full((evtLen,log_pi.shape[0]),log_pi)
    insertSeq = jax.random.categorical (insKey, jnp.array(insProbs))
    insertPos = np.full (evtLen, -1)
    insertion = jnp.array([insertSeq,insertPos]).transpose()
    t = t - jax.random.exponential(timeKey) / totalRate
    if t > 0:
      if isInsert:
        seq = jnp.insert(seq,pos,insertion,0)
#        print("inserting ",insertSeq," before position ",pos)
      else:
        seq = jnp.delete(seq,slice(pos,pos+jnp.minimum(seqLen,evtLen)),0)
#        print("deleting ",evtLen," from position ",pos)
    rngKey = nextKey
  nextAncPos = 0
  align = []
  for pos in range(seq.shape[0]):
    if seq[pos][1] >= 0:
      while nextAncPos < seq[pos][1]:
        align = align + [(int(anc[nextAncPos]),None)]
        nextAncPos = nextAncPos + 1
      align = align + [(int(anc[nextAncPos]),int(seq[pos][0]))]
      nextAncPos = nextAncPos + 1
    else:
      align = align + [(None,int(seq[pos][0]))]
  while nextAncPos < anc.shape[0]:
    align = align + [(int(anc[nextAncPos]),None)]
    nextAncPos = nextAncPos + 1
  return align

def simulated_alignment_str (align, alph):
  rows = [[col[row] for col in align] for row in [0,1]]
  ungapped = [filter(lambda col:col!=None,row) for row in rows]
  row2str = lambda row: ''.join(['-' if col==None else alph[col] for col in row])
  return row2str(ungapped[0]), row2str(ungapped[1]), [row2str(row) for row in rows]

# Constraints via alternate parameterization
# Probabilities:
#  x = 1 - exp(-|x_prime|)
#  y = 1 - exp(-|y_prime|)
# All rates are absolute values, e.g.
#  lambda = |lambda_prime|
#  etc.

# Convert parameters that we use into the form that solvers will like
def project_params (params_prime):
  indelParams_prime, substRateMatrix_prime = params_prime
  lam_prime, mu_prime, x_prime, y_prime = indelParams_prime
  indelParams = jnp.abs(lam_prime), jnp.abs(mu_prime), 1 - jnp.exp(-jnp.abs(x_prime)), 1 - jnp.exp(-jnp.abs(y_prime))
  substRateMatrix = jnp.abs (substRateMatrix_prime)
  params = indelParams, substRateMatrix
  return params

# Convert solver parameters into the form we use
def unproject_params (params):
  indelParams, substRateMatrix = params
  lam, mu, x, y = indelParams
  indelParams_prime = jnp.array ([lam, mu, -jnp.log(1-x), -jnp.log(1-y)])
  params_prime = indelParams_prime, substRateMatrix
  return params_prime

# Wrap our likelihood function, projecting parameters
def projected_likelihood (likelihood):
  return lambda params_prime: likelihood(project_params(params_prime))

# Run solver
def find_ml_params (likelihood, initParams):
  solver = jaxopt.LBFGS (projected_likelihood (likelihood), verbose=True)
  result = solver.run (unproject_params (initParams))
  return project_params (result.params)
